{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "training_on_cloud.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0e91684"
      },
      "source": [
        "# Train model on colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kk17/supermark_det/blob/main/notebooks/training_on_cloud.ipynb)\n"
      ],
      "id": "a0e91684"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a44028b",
        "outputId": "4814dacc-4360-44c1-97ac-218205cf1091"
      },
      "source": [
        "%%bash\n",
        "# clone project repo\n",
        "git clone https://github.com/kk17/supermark_det\n",
        "cd supermark_det\n",
        "./init.sh\n"
      ],
      "id": "2a44028b",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.13.3-py3-none-any.whl (1.1 MB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r /content/supermark_det/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from -r /content/supermark_det/requirements.txt (line 3)) (5.1.3)\n",
            "Collecting plotly<5,>=4.14\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.16.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.19.5)\n",
            "Collecting motor<3,>=2.3\n",
            "  Downloading motor-2.5.1-py3-none-any.whl (55 kB)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "Requirement already satisfied: pymongo<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.12.0)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting fiftyone-brain<0.8,>=0.7\n",
            "  Downloading fiftyone_brain-0.7.1-cp37-cp37m-manylinux1_x86_64.whl (631 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.60-py3-none-any.whl (131 kB)\n",
            "Collecting voxel51-eta<0.6,>=0.5.3\n",
            "  Downloading voxel51_eta-0.5.3-py2.py3-none-any.whl (557 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.8.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (5.4.8)\n",
            "Collecting mongoengine==0.20.0\n",
            "  Downloading mongoengine-0.20.0-py3-none-any.whl (106 kB)\n",
            "Collecting pprintpp\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: argcomplete in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.12.3)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting eventlet\n",
            "  Downloading eventlet-0.32.0-py2.py3-none-any.whl (225 kB)\n",
            "Collecting fiftyone-db<0.4,>=0.3\n",
            "  Downloading fiftyone_db-0.3.0-py3-none-manylinux1_x86_64.whl (29.2 MB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (21.0)\n",
            "Requirement already satisfied: tornado<7,>=5.1.1 in /usr/local/lib/python3.7/dist-packages (from fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (5.1.1)\n",
            "Collecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from fiftyone-brain<0.8,>=0.7->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly<5,>=4.14->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.15.0)\n",
            "Collecting httpx>=0.10.0\n",
            "  Downloading httpx-0.19.0-py3-none-any.whl (77 kB)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.7/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.0.6)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.14.0,>=0.13.3\n",
            "  Downloading httpcore-0.13.7-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2021.5.30)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "Collecting anyio==3.*\n",
            "  Downloading anyio-3.3.3-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.14.0,>=0.13.3->httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.14.0,>=0.13.3->httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2018.9)\n",
            "Collecting ndjson\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (4.8.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.41.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r /content/supermark_det/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.7/dist-packages (from nbformat->-r /content/supermark_det/requirements.txt (line 3)) (5.1.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->-r /content/supermark_det/requirements.txt (line 3)) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat->-r /content/supermark_det/requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->-r /content/supermark_det/requirements.txt (line 3)) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->voxel51-eta<0.6,>=0.5.3->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (3.6.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.60\n",
            "  Downloading botocore-1.21.60-py3-none-any.whl (8.0 MB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Collecting dnspython>=1.15.0\n",
            "  Downloading dnspython-2.1.0-py3-none-any.whl (241 kB)\n",
            "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.7/dist-packages (from eventlet->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fiftyone->-r /content/supermark_det/requirements.txt (line 1)) (1.0.1)\n",
            "Installing collected packages: urllib3, sniffio, rfc3986, jmespath, h11, anyio, httpcore, botocore, s3transfer, patool, opencv-python-headless, ndjson, httpx, dnspython, xmltodict, voxel51-eta, universal-analytics-python3, pprintpp, plotly, motor, mongoengine, kaleido, fiftyone-db, fiftyone-brain, eventlet, Deprecated, boto3, fiftyone\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "Successfully installed Deprecated-1.2.13 anyio-3.3.3 boto3-1.18.60 botocore-1.21.60 dnspython-2.1.0 eventlet-0.32.0 fiftyone-0.13.3 fiftyone-brain-0.7.1 fiftyone-db-0.3.0 h11-0.12.0 httpcore-0.13.7 httpx-0.19.0 jmespath-0.10.0 kaleido-0.2.1 mongoengine-0.20.0 motor-2.5.1 ndjson-0.3.1 opencv-python-headless-4.5.3.56 patool-1.12 plotly-4.14.3 pprintpp-0.4.0 rfc3986-1.5.0 s3transfer-0.5.0 sniffio-1.2.0 universal-analytics-python3-1.1.1 urllib3-1.25.11 voxel51-eta-0.5.3 xmltodict-0.12.0\n",
            "Cloning https://github.com/tensorflow/models\n",
            "Installing protobuf\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf) (1.15.0)\n",
            "Found protoc\n",
            "Install the Object Detection API\n",
            "Processing /content/supermark_det/tensorflow_model_garden/research\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Collecting tensorflow-text>=2.5.0\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.5.3.56)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.25.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.41.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.0)\n",
            "Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.2.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, avro-python3, dill, future, seqeval\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1667808 sha256=00efefa136b64fee78aa0b011f959496b08c18ff583ce3833d8df75cf745853f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0kxzbnga/wheels/04/fc/25/3a78d67772a8c692e6da7c236d33cbc9f28ec6bea46c1e75fd\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=463ff3472fdec3b2aae4c60b212b26fb3609425cb62b09676517214153baaead\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=ca6156a19f54d1aa10c687f34463ec27c29ec87f3dc2e149d8c486637f3ef29a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=5288c9bd4f6abf8903f385dd8eeb142cec15d020472310416c2c4b734e88c141\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8202cce5e01e83942413e2bc5f7d433b74f8e4f58c92875c8085a39ae20d4227\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=28a61b23d05532fd208328544416574b924baf6ed1920205076f21f01130d0aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo avro-python3 dill future seqeval\n",
            "Installing collected packages: requests, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, hdfs, fastavro, avro-python3, tf-models-official, lvis, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed apache-beam-2.33.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.5 future-0.18.2 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 orjson-3.6.4 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-5.4.1 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.14.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0\n",
            "Installation complete\n",
            "Copy scripts for training and exporting model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'supermark_det'...\n",
            "+ XTRACE=false\n",
            "+ [[ false = \\t\\r\\u\\e ]]\n",
            "+ IFS='\n",
            "\t'\n",
            "+++ dirname ./init.sh\n",
            "++ cd .\n",
            "++ pwd\n",
            "+ DIR=/content/supermark_det\n",
            "+ pip install -r /content/supermark_det/requirements.txt\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "+ TENSORFLOW_MODEL_GARDEN_DIR=/content/supermark_det/tensorflow_model_garden\n",
            "+ WORKSPACE_DIR=/content/supermark_det/workspace\n",
            "+ SKIP_CLONE=false\n",
            "+ getopts s FLAG\n",
            "+ command -v git\n",
            "+ '[' false = false ']'\n",
            "+ '[' -d /content/supermark_det/tensorflow_model_garden ']'\n",
            "+ echo 'Cloning https://github.com/tensorflow/models'\n",
            "+ git clone --depth 1 https://github.com/tensorflow/models /content/supermark_det/tensorflow_model_garden\n",
            "Cloning into '/content/supermark_det/tensorflow_model_garden'...\n",
            "+ cd /content/supermark_det/tensorflow_model_garden\n",
            "+ echo 'Installing protobuf'\n",
            "+ pip install protobuf\n",
            "+ command -v protoc\n",
            "+ echo 'Found protoc'\n",
            "+ echo 'Install the Object Detection API'\n",
            "+ cd research/\n",
            "+ protoc object_detection/protos/anchor_generator.proto object_detection/protos/argmax_matcher.proto object_detection/protos/bipartite_matcher.proto object_detection/protos/box_coder.proto object_detection/protos/box_predictor.proto object_detection/protos/calibration.proto object_detection/protos/center_net.proto object_detection/protos/eval.proto object_detection/protos/faster_rcnn_box_coder.proto object_detection/protos/faster_rcnn.proto object_detection/protos/flexible_grid_anchor_generator.proto object_detection/protos/fpn.proto object_detection/protos/graph_rewriter.proto object_detection/protos/grid_anchor_generator.proto object_detection/protos/hyperparams.proto object_detection/protos/image_resizer.proto object_detection/protos/input_reader.proto object_detection/protos/keypoint_box_coder.proto object_detection/protos/losses.proto object_detection/protos/matcher.proto object_detection/protos/mean_stddev_box_coder.proto object_detection/protos/model.proto object_detection/protos/multiscale_anchor_generator.proto object_detection/protos/optimizer.proto object_detection/protos/pipeline.proto object_detection/protos/post_processing.proto object_detection/protos/preprocessor.proto object_detection/protos/region_similarity_calculator.proto object_detection/protos/square_box_coder.proto object_detection/protos/ssd_anchor_generator.proto object_detection/protos/ssd.proto object_detection/protos/string_int_label_map.proto object_detection/protos/target_assigner.proto object_detection/protos/train.proto --python_out=.\n",
            "+ cp object_detection/packages/tf2/setup.py .\n",
            "+ python -m pip install .\n",
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "+ echo 'Installation complete'\n",
            "+ echo 'Copy scripts for training and exporting model'\n",
            "+ SCRIPT_PATH=research/object_detection/model_main_tf2.py\n",
            "+ cp /content/supermark_det/tensorflow_model_garden/research/object_detection/model_main_tf2.py /content/supermark_det/workspace\n",
            "+ SCRIPT_PATH=research/object_detection/exporter_main_v2.py\n",
            "+ cp /content/supermark_det/tensorflow_model_garden/research/object_detection/exporter_main_v2.py /content/supermark_det/workspace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296f20f2"
      },
      "source": [
        "# check installation\n",
        "import tensorflow.compat.v2 as tf\n",
        "from google.protobuf import text_format\n",
        "from object_detection import exporter_lib_v2\n",
        "from object_detection.protos import pipeline_pb2"
      ],
      "id": "296f20f2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ea8296"
      },
      "source": [
        "## mount drive"
      ],
      "id": "08ea8296"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a5b61ac",
        "outputId": "81ed38d5-f0ec-4b07-fe8f-240f9145e132"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "3a5b61ac",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a19da1a2"
      },
      "source": [
        "!rsync -r /content/drive/MyDrive/supermarket_detection_workspace/ /content/supermark_det/workspace"
      ],
      "id": "a19da1a2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35dd9c19"
      },
      "source": [
        "## Use colabcode"
      ],
      "id": "35dd9c19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "187ca463",
        "outputId": "5e2a6323-f81d-4440-eb5b-3de7edfce004"
      },
      "source": [
        "!pip install colabcode"
      ],
      "id": "187ca463",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colabcode\n",
            "  Downloading colabcode-0.3.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting uvicorn==0.13.1\n",
            "  Downloading uvicorn-0.13.1-py3-none-any.whl (45 kB)\n",
            "\u001b[K     || 45 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting jupyterlab==3.0.7\n",
            "  Downloading jupyterlab-3.0.7-py3-none-any.whl (8.3 MB)\n",
            "\u001b[K     || 8.3 MB 6.2 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio==1.4.3\n",
            "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
            "Collecting pyngrok>=5.0.0\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     || 745 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.1.0\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     || 428 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (4.8.1)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (2.11.3)\n",
            "Collecting jupyterlab-server~=2.0\n",
            "  Downloading jupyterlab_server-2.8.2-py3-none-any.whl (58 kB)\n",
            "\u001b[K     || 58 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (21.0)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading nbclassic-0.3.2-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (5.5.0)\n",
            "Collecting jupyter-server~=1.2\n",
            "  Downloading jupyter_server-1.11.1-py3-none-any.whl (393 kB)\n",
            "\u001b[K     || 393 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10->jupyterlab==3.0.7->colabcode) (2.0.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.1.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.11.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (3.3.3)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (21.1.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.1.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (22.3.0)\n",
            "Collecting jupyter-client>=6.1.1\n",
            "  Downloading jupyter_client-7.0.6-py3-none-any.whl (125 kB)\n",
            "\u001b[K     || 125 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting requests-unixsocket\n",
            "  Downloading requests_unixsocket-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     || 52 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.10)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.8.2)\n",
            "Collecting jupyter-client>=6.1.1\n",
            "  Downloading jupyter_client-7.0.5-py3-none-any.whl (124 kB)\n",
            "\u001b[K     || 124 kB 63.3 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-7.0.4-py3-none-any.whl (124 kB)\n",
            "\u001b[K     || 124 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-7.0.3-py3-none-any.whl (122 kB)\n",
            "\u001b[K     || 122 kB 61.0 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-7.0.2-py3-none-any.whl (122 kB)\n",
            "\u001b[K     || 122 kB 65.5 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-7.0.1-py3-none-any.whl (122 kB)\n",
            "\u001b[K     || 122 kB 66.0 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-7.0.0-py3-none-any.whl (122 kB)\n",
            "\u001b[K     || 122 kB 66.4 MB/s \n",
            "\u001b[?25h  Downloading jupyter_client-6.1.12-py3-none-any.whl (112 kB)\n",
            "\u001b[K     || 112 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting jsonschema>=3.0.1\n",
            "  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     || 69 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.26.0)\n",
            "Collecting json5\n",
            "  Downloading json5-0.9.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.9.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (21.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (4.8.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (0.18.0)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (4.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok>=5.0.0->colabcode) (5.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.1->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.20)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab==3.0.7->colabcode) (0.2.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->jupyterlab==3.0.7->colabcode) (2.4.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (1.25.11)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2021.5.30)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=64f4665718f18e5394603ed5e71ebf4cdc5fe18ae8c9ef916352eafbb85d8783\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: tornado, jsonschema, jupyter-client, websocket-client, requests-unixsocket, jupyter-server, json5, nbclassic, jupyterlab-server, uvicorn, pyngrok, nest-asyncio, jupyterlab, colabcode\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.5.1\n",
            "    Uninstalling nest-asyncio-1.5.1:\n",
            "      Successfully uninstalled nest-asyncio-1.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed colabcode-0.3.0 json5-0.9.6 jsonschema-4.1.0 jupyter-client-6.1.12 jupyter-server-1.11.1 jupyterlab-3.0.7 jupyterlab-server-2.8.2 nbclassic-0.3.2 nest-asyncio-1.4.3 pyngrok-5.1.0 requests-unixsocket-0.2.0 tornado-6.1 uvicorn-0.13.1 websocket-client-1.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jupyter_client",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9d3e48",
        "outputId": "2022364a-573c-44e9-e037-6a0680c79121"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "\n",
        "# vscode\n",
        "ColabCode(port=10000)\n",
        "# ColabCode(port=10000, password=\"k8789fasdfad\")\n",
        "# jupyter lab\n",
        "# ColabCode(port=10000, lab=True)"
      ],
      "id": "9a9d3e48",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"client session established\" obj=csess id=67cfc4652f5d\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=start pg=/api/tunnels id=ff7341c7d3a2242c\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=end pg=/api/tunnels id=ff7341c7d3a2242c status=200 dur=419.273s\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=start pg=/api/tunnels id=226fa5c313169606\n",
            "Opening tunnel named: http-10000-23346b47-6a84-49bc-9d74-287fcd30541f\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=end pg=/api/tunnels id=226fa5c313169606 status=200 dur=136.844s\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=start pg=/api/tunnels id=4986e9707e53462f\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=end pg=/api/tunnels id=4986e9707e53462f status=200 dur=140.066s\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=start pg=/api/tunnels id=78555a04992dc112\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-10000-23346b47-6a84-49bc-9d74-287fcd30541f addr=http://localhost:10000 url=https://dd88-35-237-246-1.ngrok.io\n",
            "Code Server can be accessed on: NgrokTunnel: \"https://dd88-35-237-246-1.ngrok.io\" -> \"http://localhost:10000\"\n",
            "t=2021-10-13T12:44:07+0000 lvl=info msg=end pg=/api/tunnels id=78555a04992dc112 status=201 dur=49.375553ms\n",
            "[2021-10-13T12:44:08.493Z] info  code-server 3.10.2 387b12ef4ca404ffd39d84834e1f0776e9e3c005\n",
            "[2021-10-13T12:44:08.496Z] info  Using user-data-dir ~/.local/share/code-server\n",
            "[2021-10-13T12:44:08.511Z] info  Using config file ~/.config/code-server/config.yaml\n",
            "[2021-10-13T12:44:08.511Z] info  HTTP server listening on http://127.0.0.1:10000 \n",
            "[2021-10-13T12:44:08.511Z] info    - Authentication is disabled \n",
            "[2021-10-13T12:44:08.511Z] info    - Not serving HTTPS \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0326cc1d"
      },
      "source": [
        "!rsync -r  /content/supermark_det/workspace/models/ /content/drive/MyDrive/supermarket_detection_workspace/models"
      ],
      "id": "0326cc1d",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPIRt7Uw30kN"
      },
      "source": [
        "!rsync -r  /content/supermark_det/workspace/exported_models/ /content/drive/MyDrive/supermarket_detection_workspace/exported_models"
      ],
      "id": "zPIRt7Uw30kN",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293357a1"
      },
      "source": [
        "# install fiftyone\n",
        "\n",
        "[FiftyOne](https://voxel51.com/docs/fiftyone/index.html) is open-source tool for building high-quality datasets and computer vision models.\n"
      ],
      "id": "293357a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eec2370"
      },
      "source": [
        "!pip install fiftyone\n"
      ],
      "id": "4eec2370",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99077de3",
        "outputId": "e3e2ac39-44ed-4975-9bec-4ca17310829a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from fiftyone.core import dataset as D\n",
        "from fiftyone import ViewField as F\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n"
      ],
      "id": "99077de3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f50479"
      },
      "source": [
        "# Functions to build customized dataset\n"
      ],
      "id": "09f50479"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5e7dfe1"
      },
      "source": [
        "classes = [\"apple\", \"orange\", \"banana\", \"book\"]\n",
        "oi_classes = [c[0].upper() + c[1:] for c in classes]"
      ],
      "id": "b5e7dfe1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802fea18"
      },
      "source": [
        "def build_dataset(\n",
        "    classes,\n",
        "    source_dataset='open-images-v6',\n",
        "    split=\"validation\",\n",
        "    label_types=['detections'],\n",
        "    max_samples_per_class=10,\n",
        "    delete_exist_dataset=False\n",
        "    ):\n",
        "\n",
        "    all_calss_dataset_name = f'{source_dataset}-{split}-{max_samples_per_class}'\n",
        "    dataset = None\n",
        "    if D.dataset_exists(all_calss_dataset_name):\n",
        "        if delete_exist_dataset:\n",
        "            print(f'Delete existing dataset {all_calss_dataset_name}')\n",
        "            D.delete_dataset(all_calss_dataset_name)\n",
        "        else:\n",
        "            print(f'Load existing dataset {all_calss_dataset_name}')\n",
        "            dataset = D.load_dataset(all_calss_dataset_name)\n",
        "    if dataset:\n",
        "        return dataset\n",
        "\n",
        "    single_class_datasets = []\n",
        "    for c in classes:\n",
        "        dataset_name = f'{source_dataset}-{split}-{c}-{max_samples_per_class}'\n",
        "        single_class_dataset = None\n",
        "        if D.dataset_exists(dataset_name):\n",
        "            if delete_exist_dataset:\n",
        "                print(f'Delete existing dataset {dataset_name}')\n",
        "                D.delete_dataset(dataset_name)\n",
        "            else:\n",
        "                print(f'Load existing dataset {dataset_name}')\n",
        "                single_class_dataset = D.load_dataset(dataset_name)\n",
        "\n",
        "        if not single_class_dataset:\n",
        "            single_class_dataset = foz.load_zoo_dataset(\n",
        "                source_dataset,\n",
        "                split=split,\n",
        "                label_types=label_types,\n",
        "                classes=[c], \n",
        "                max_samples=max_samples_per_class,\n",
        "                dataset_name=dataset_name\n",
        "            )\n",
        "        single_class_datasets.append(single_class_dataset)\n",
        "\n",
        "    dataset = single_class_datasets[0].clone(all_calss_dataset_name)\n",
        "    for d in single_class_datasets[1:]:\n",
        "        dataset.merge_samples(d)\n",
        "    return dataset\n"
      ],
      "id": "802fea18",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b6b017c"
      },
      "source": [
        "# Bboxes are in [top-left-x, top-left-y, width, height] format\n",
        "bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
        "\n",
        "\n",
        "def build_dateset_view(dataset,\n",
        "                       dataset_type='open_image',\n",
        "                       IsOccluded=None,\n",
        "                       IsTruncated=None,\n",
        "                       IsGroupOf=None,\n",
        "                       IsDepiction=None,\n",
        "                       IsInside=None,\n",
        "                       iscrowd=None,\n",
        "                       valid_labels=oi_classes,\n",
        "                       bbox_area_lower_bound=None):\n",
        "    if dataset_type =='open_image':\n",
        "        gt_field = 'detections'\n",
        "    else:\n",
        "        gt_field = 'ground_truth'\n",
        "    view = dataset\n",
        "    if valid_labels:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"label\").is_in(valid_labels),\n",
        "                                  only_matches=False)\n",
        "    if IsOccluded is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"IsOccluded\") == IsOccluded,\n",
        "                                  only_matches=True)\n",
        "    if IsTruncated is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"IsTruncated\") == IsTruncated,\n",
        "                                  only_matches=True)\n",
        "    if IsGroupOf is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"IsGroupOf\") == IsGroupOf,\n",
        "                                  only_matches=True)\n",
        "    if IsDepiction is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"IsDepiction\") == IsDepiction,\n",
        "                                  only_matches=True)\n",
        "    if IsInside is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"IsInside\") == IsInside,\n",
        "                                  only_matches=True)\n",
        "    if iscrowd is not None:\n",
        "        view = view.filter_labels(gt_field,\n",
        "                                  F(\"iscrowd\") == iscrowd,\n",
        "                                  only_matches=True)\n",
        "    if bbox_area_lower_bound:\n",
        "        view = view.filter_labels(\n",
        "            gt_field,\n",
        "            bbox_area > bbox_area_lower_bound,\n",
        "        )\n",
        "    return view\n"
      ],
      "id": "0b6b017c",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f727c488"
      },
      "source": [
        "def export_dataset_or_view(\n",
        "    dataset_or_view,\n",
        "    export_dir,\n",
        "    label_field,\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    overwrite=False,\n",
        "    tf_file_name=None,\n",
        "):\n",
        "    dataset_or_view.export(\n",
        "        export_dir=export_dir,\n",
        "        dataset_type=dataset_type,\n",
        "        label_field=label_field,\n",
        "        overwrite=overwrite\n",
        "    )\n",
        "\n",
        "    if dataset_type == fo.types.TFObjectDetectionDataset and tf_file_name:\n",
        "        if not tf_file_name.endswith('.records'):\n",
        "            tf_file_name += '.records'\n",
        "        source = os.path.join(export_dir, 'tf.records')\n",
        "        target = os.path.join(export_dir, tf_file_name)\n",
        "        if os.path.exists(target) and overwrite:\n",
        "            os.remove(target)\n",
        "        os.rename(source, target)"
      ],
      "id": "f727c488",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8487c615"
      },
      "source": [
        "def check_tf_record(filepath, take_num=1):\n",
        "    raw_dataset = tf.data.TFRecordDataset(filepath)\n",
        "    for raw_record in raw_dataset.take(take_num):\n",
        "        example = tf.train.Example()\n",
        "        example.ParseFromString(raw_record.numpy())\n",
        "        print(example)\n",
        "        # print('-' * 32)\n",
        "        # n = len(example.features)\n",
        "        # for i in range(n):\n",
        "        #     feature = example.features[i]\n",
        "        #     if feature.key != 'image/encoded':\n",
        "        #         print(feature)\n"
      ],
      "id": "8487c615",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffd6e6f1"
      },
      "source": [
        "# Open Image Dataset\n",
        "\n",
        "[fiftyone_open_images.ipynb - Colaboratory](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.13.3/docs/source/tutorials/open_images.ipynb#scrollTo=4TvnIN7xRtuh)\n"
      ],
      "id": "ffd6e6f1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c43ee23c",
        "outputId": "dfc4b5fb-8a75-45b3-c19c-916c65e33577"
      },
      "source": [
        "ds = build_dataset(oi_classes, delete_exist_dataset=False)"
      ],
      "id": "c43ee23c",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load existing dataset open-images-v6-validation-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2179eb4a"
      },
      "source": [
        "session = fo.launch_app(ds)"
      ],
      "id": "2179eb4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a1c2c50"
      },
      "source": [
        "view  = build_dateset_view(ds, IsGroupOf=False, bbox_area_lower_bound=0.1)\n",
        "session = fo.launch_app(view=view)"
      ],
      "id": "9a1c2c50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca36b056",
        "outputId": "1b04e3d7-924c-43ad-b83d-a994a2cc7954"
      },
      "source": [
        "counts = view.count_values(\"detections.detections.label\")\n",
        "counts"
      ],
      "id": "ca36b056",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Banana': 1, 'Orange': 9, 'Apple': 7, 'Book': 1}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c51e249"
      },
      "source": [
        "train_ds = build_dataset(oi_classes, split='train', max_samples_per_class=1000, delete_exist_dataset=False)\n",
        "train_filterd_view  = build_dateset_view(train_ds, IsGroupOf=False, bbox_area_lower_bound=0.1)\n",
        "counts = train_filterd_view.count_values(\"detections.detections.label\")\n",
        "print(counts)\n",
        "session = fo.launch_app(view=train_filterd_view)"
      ],
      "id": "6c51e249",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39b024cc",
        "outputId": "e13ecac4-5f2d-4815-b237-8a8ccbf8ff3f"
      },
      "source": [
        "export_dataset_or_view(train_filterd_view,\n",
        "                       '../workspace/data/open_image_train_filtered_as_coco',\n",
        "                       label_field='detections')\n"
      ],
      "id": "39b024cc",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% || 1446/1446 [27.5s elapsed, 0s remaining, 45.1 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2caa474",
        "outputId": "e92f5a69-ab5f-4284-f2a3-cc1559fe7232"
      },
      "source": [
        "export_dataset_or_view(train_filterd_view,\n",
        "                       '../workspace/data',\n",
        "                       label_field='detections',\n",
        "                       tf_file_name='train',\n",
        "                       dataset_type=fo.types.TFObjectDetectionDataset)\n"
      ],
      "id": "a2caa474",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0% |/--------------|    6/1446 [118.3ms elapsed, 28.4s remaining, 50.7 samples/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-10 16:05:34.230313: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% || 1446/1446 [18.9s elapsed, 0s remaining, 81.3 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b82bb733",
        "outputId": "88a2c6bc-4f2f-45e3-dead-e0c7bbc4db98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "open_image_test_ds = build_dataset(oi_classes,\n",
        "                                   split='test',\n",
        "                                   max_samples_per_class=1000,\n",
        "                                   delete_exist_dataset=False)\n",
        "open_image_test_filterd_view = build_dateset_view(open_image_test_ds,\n",
        "                                                  IsGroupOf=False,\n",
        "                                                  bbox_area_lower_bound=0.1)\n",
        "counts = open_image_test_filterd_view.count_values(\n",
        "    \"detections.detections.label\")\n",
        "print(counts)\n",
        "open_image_test_filterd_view.save()\n",
        "# session = fo.launch_app(view=open_image_test_filterd_view)\n",
        "# export_dataset_or_view(\n",
        "#     open_image_test_filterd_view,\n",
        "#     '../workspace/data/test',\n",
        "#     label_field='detections',\n",
        "#     dataset_type=fo.types.TFObjectDetectionDataset)\n"
      ],
      "id": "b82bb733",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'test' to '/root/fiftyone/open-images-v6/test' if necessary\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to '/root/fiftyone/open-images-v6/test/metadata/image_ids.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v6/test/metadata/classes.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpebwyv85e/metadata/hierarchy.json'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to '/root/fiftyone/open-images-v6/test/labels/detections.csv'\n",
            "Only found 167 (<1000) samples matching your requirements\n",
            "Downloading 167 images\n",
            " 100% || 167/167 [6.8s elapsed, 0s remaining, 28.8 images/s]       \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'test'\n",
            " 100% || 167/167 [1.5s elapsed, 0s remaining, 115.3 samples/s]         \n",
            "Dataset 'open-images-v6-test-Apple-1000' created\n",
            "Downloading split 'test' to '/root/fiftyone/open-images-v6/test' if necessary\n",
            "Only found 230 (<1000) samples matching your requirements\n",
            "Found 4 images, downloading the remaining 226\n",
            " 100% || 226/226 [9.3s elapsed, 0s remaining, 20.2 images/s]       \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'test'\n",
            " 100% || 230/230 [3.4s elapsed, 0s remaining, 63.8 samples/s]      \n",
            "Dataset 'open-images-v6-test-Orange-1000' created\n",
            "Downloading split 'test' to '/root/fiftyone/open-images-v6/test' if necessary\n",
            "Only found 76 (<1000) samples matching your requirements\n",
            "Found 2 images, downloading the remaining 74\n",
            " 100% || 74/74 [3.4s elapsed, 0s remaining, 27.0 images/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'test'\n",
            " 100% || 76/76 [609.7ms elapsed, 0s remaining, 126.2 samples/s]      \n",
            "Dataset 'open-images-v6-test-Banana-1000' created\n",
            "Downloading split 'test' to '/root/fiftyone/open-images-v6/test' if necessary\n",
            "Only found 314 (<1000) samples matching your requirements\n",
            "Downloading 314 images\n",
            " 100% || 314/314 [13.3s elapsed, 0s remaining, 24.9 images/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'test'\n",
            " 100% || 314/314 [7.9s elapsed, 0s remaining, 40.7 samples/s]       \n",
            "Dataset 'open-images-v6-test-Book-1000' created\n",
            "{'Book': 144, 'Apple': 109, 'Orange': 164, 'Banana': 36}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d4b91be",
        "outputId": "357a17fc-2be0-476b-ff5f-cf852dc35056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "open_image_validation_ds = build_dataset(oi_classes,\n",
        "                                   split='validation',\n",
        "                                   max_samples_per_class=1000,\n",
        "                                   delete_exist_dataset=False)\n",
        "open_image_validation_filterd_view = build_dateset_view(open_image_validation_ds,\n",
        "                                                  IsGroupOf=False,\n",
        "                                                  bbox_area_lower_bound=0.1)\n",
        "counts = open_image_validation_filterd_view.count_values(\n",
        "    \"detections.detections.label\")\n",
        "print(counts)\n",
        "open_image_validation_filterd_view.save()\n",
        "# session = fo.launch_app(view=open_image_validation_filterd_view)\n",
        "# export_dataset_or_view(\n",
        "#     open_image_validation_filterd_view,\n",
        "#     '../workspace/data',\n",
        "#     tf_file_name='validation',\n",
        "#     label_field='detections',\n",
        "#     dataset_type=fo.types.TFObjectDetectionDataset)\n"
      ],
      "id": "7d4b91be",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/open-images-v6/validation' if necessary\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to '/root/fiftyone/open-images-v6/validation/metadata/image_ids.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v6/validation/metadata/classes.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmp34vw4yar/metadata/hierarchy.json'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv' to '/root/fiftyone/open-images-v6/validation/labels/detections.csv'\n",
            "Only found 54 (<1000) samples matching your requirements\n",
            "Downloading 54 images\n",
            " 100% || 54/54 [2.8s elapsed, 0s remaining, 24.4 images/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'validation'\n",
            " 100% || 54/54 [375.1ms elapsed, 0s remaining, 147.6 samples/s]      \n",
            "Dataset 'open-images-v6-validation-Apple-1000' created\n",
            "Downloading split 'validation' to '/root/fiftyone/open-images-v6/validation' if necessary\n",
            "Only found 62 (<1000) samples matching your requirements\n",
            "Downloading 62 images\n",
            " 100% || 62/62 [2.9s elapsed, 0s remaining, 26.7 images/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'validation'\n",
            " 100% || 62/62 [612.5ms elapsed, 0s remaining, 101.2 samples/s]      \n",
            "Dataset 'open-images-v6-validation-Orange-1000' created\n",
            "Downloading split 'validation' to '/root/fiftyone/open-images-v6/validation' if necessary\n",
            "Only found 18 (<1000) samples matching your requirements\n",
            "Found 1 images, downloading the remaining 17\n",
            " 100% || 17/17 [1.2s elapsed, 0s remaining, 14.2 images/s]         \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'validation'\n",
            " 100% || 18/18 [158.3ms elapsed, 0s remaining, 113.7 samples/s]     \n",
            "Dataset 'open-images-v6-validation-Banana-1000' created\n",
            "Downloading split 'validation' to '/root/fiftyone/open-images-v6/validation' if necessary\n",
            "Only found 95 (<1000) samples matching your requirements\n",
            "Downloading 95 images\n",
            " 100% || 95/95 [4.1s elapsed, 0s remaining, 26.2 images/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'validation'\n",
            " 100% || 95/95 [2.5s elapsed, 0s remaining, 38.7 samples/s]      \n",
            "Dataset 'open-images-v6-validation-Book-1000' created\n",
            "{'Apple': 34, 'Orange': 47, 'Banana': 6, 'Book': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874e1fe2"
      },
      "source": [
        "check_tf_record('../workspace/data/validation.records')"
      ],
      "id": "874e1fe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07a54eaf"
      },
      "source": [
        "sample = dataset.first()\n",
        "print(sample)"
      ],
      "id": "07a54eaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5829fe05"
      },
      "source": [
        "sample['segmentations']['detections'][0]"
      ],
      "id": "5829fe05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "774ed487"
      },
      "source": [
        "from fiftyone import ViewField as F\n",
        "# TODO get samples with desired classes and not crowed\n",
        "# Sort samples with the less objects first\n",
        "obj_count_view = dataset.sort_by(F(\"ground_truth.detections\").length(),\n",
        "                                 reverse=False)\n",
        "obj_count_view = dataset.sort_by(F(\"detections.detections\").length(), reverse=False)\n",
        "\n",
        "session.view = obj_count_view"
      ],
      "id": "774ed487",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d554cdc"
      },
      "source": [
        ""
      ],
      "id": "2d554cdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd002c6d"
      },
      "source": [
        "# Check with coco2017 dataset\n",
        "\n",
        "[What Object Categories / Labels Are In COCO Dataset? | Amikelive | Technology Blog](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/)\n"
      ],
      "id": "bd002c6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "https://localhost:5151/polling?sessionId=53fddf64-3075-4a7c-8ee2-5d4d2c333f25": {
              "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
              "headers": [
                [
                  "access-control-allow-headers",
                  "x-requested-with"
                ],
                [
                  "content-type",
                  "text/html; charset=UTF-8"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "c37f8629",
        "outputId": "ed75904e-0baf-469d-e023-cf86aefb153f"
      },
      "source": [
        "coco_train_ds = build_dataset(classes,\n",
        "                              source_dataset='coco-2017',\n",
        "                              split='train',\n",
        "                              max_samples_per_class=1000)\n",
        "coco_train_filterd_view = build_dateset_view(coco_train_ds,\n",
        "                                             dataset_type='coco',\n",
        "                                             valid_labels=classes,\n",
        "                                             iscrowd=0,\n",
        "                                             bbox_area_lower_bound=0.1)\n",
        "counts = coco_train_filterd_view.count_values(\"ground_truth.detections.label\")\n",
        "print(counts)\n",
        "session = fo.launch_app(view=coco_train_filterd_view)"
      ],
      "id": "c37f8629",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load existing dataset coco-2017-train-1000\n",
            "{'orange': 497, 'apple': 546, 'banana': 843, 'book': 148}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"800\"\n",
              "            src=\"http://localhost:5151/?notebook=true&handleId=af4ecb41-9bb1-4db3-ab05-9874bd0cc19e\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fc7803740d0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad497216",
        "outputId": "1168f9e8-db70-46da-aff3-a2a29d814ef4"
      },
      "source": [
        "coco_train_ds"
      ],
      "id": "ad497216",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Name:        coco-2017-train-1000\n",
              "Media type:  image\n",
              "Num samples: 3066\n",
              "Persistent:  False\n",
              "Tags:        ['train']\n",
              "Sample fields:\n",
              "    id:           fiftyone.core.fields.ObjectIdField\n",
              "    filepath:     fiftyone.core.fields.StringField\n",
              "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
              "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49cfa6be"
      },
      "source": [
        "sample = coco_train_ds.first()\n",
        "print(sample)"
      ],
      "id": "49cfa6be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93db0d73"
      },
      "source": [
        "from fiftyone import ViewField as F\n",
        "# TODO get samples with desired classes and not crowed\n",
        "# Sort samples with the less objects first\n",
        "obj_count_view = dataset.sort_by(F(\"ground_truth.detections\").length(), reverse=False)\n",
        "\n",
        "session.view = obj_count_view"
      ],
      "id": "93db0d73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68604da2"
      },
      "source": [
        "### Use this Jupyter Notebook as a guide to run your trained model in inference mode\n",
        "\n",
        "created by Anton Morgunov\n",
        "\n",
        "inspired by [notebooks object detection API tutorial](https://notebooks-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model)"
      ],
      "id": "68604da2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb50004"
      },
      "source": [
        "Your first step is going to specify which unit you are going to work with for inference. Select between GPU or CPU and follow the below instructions for implementation."
      ],
      "id": "9fb50004"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f61713ba"
      },
      "source": [
        "import os # importing OS in order to make GPU visible\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # do not change anything in here\n",
        "\n",
        "# specify which device you want to work on.\n",
        "# Use \"-1\" to work on a CPU. Default value \"0\" stands for the 1st GPU that will be used\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # TODO: specify your computational device"
      ],
      "id": "f61713ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ccff1bd",
        "outputId": "0ad3df34-d3d6-4f39-8b73-cc9110efec5f"
      },
      "source": [
        "import notebooks as tf # import notebooks\n",
        "\n",
        "# checking that GPU is found\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ],
      "id": "0ccff1bd",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-10 19:28:39.270406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f28f30d"
      },
      "source": [
        "# other import\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "id": "6f28f30d",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6667579"
      },
      "source": [
        "Next you will import import scripts that were already provided by notebooks API. **Make sure that notebooks is your current working directory.**"
      ],
      "id": "a6667579"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afae6678"
      },
      "source": [
        "import sys # importyng sys in order to access scripts located in a different folder\n",
        "\n",
        "path2scripts = 'models/research/' # TODO: provide pass to the research folder\n",
        "sys.path.insert(0, path2scripts) # making scripts in models/research available for import"
      ],
      "id": "afae6678",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cf7b92e"
      },
      "source": [
        "# importing all scripts that will be needed to export your model and use it for inference\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder"
      ],
      "id": "3cf7b92e",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44c779a"
      },
      "source": [
        "Now you can import and build your trained model:"
      ],
      "id": "c44c779a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ac177df"
      },
      "source": [
        "# NOTE: your current working directory should be notebooks.\n",
        "\n",
        "# specify two pathes: to the pipeline.config file and to the folder with trained model.\n",
        "path2config ='/content/supermark_det/workspace/exported_models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/v1/pipeline.config'\n",
        "path2model = '/content/supermark_det/workspace/exported_models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/v1'"
      ],
      "id": "6ac177df",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99e4abd"
      },
      "source": [
        "# do not change anything in this cell\n",
        "configs = config_util.get_configs_from_pipeline_file(path2config) # importing config\n",
        "model_config = configs['model'] # recreating model config\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"
      ],
      "id": "a99e4abd",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cde63284",
        "outputId": "fae34912-a9ea-4f00-fe52-55ca3757870f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(path2model, 'checkpoint', 'ckpt-0')).expect_partial()"
      ],
      "id": "cde63284",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcfcc92e690>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f1173e"
      },
      "source": [
        "Next, path to label map should be provided. Category index will be created based on labal map file"
      ],
      "id": "29f1173e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "398d1e8a"
      },
      "source": [
        "path2label_map = '/content/supermark_det/workspace/data/label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(path2label_map,use_display_name=True)"
      ],
      "id": "398d1e8a",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fc4b39"
      },
      "source": [
        "Now, a few supporting functions will be defined"
      ],
      "id": "97fc4b39"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007c9c4b"
      },
      "source": [
        "def detect_fn(image):\n",
        "    \"\"\"\n",
        "    Detect objects in image.\n",
        "    \n",
        "    Args:\n",
        "      image: (tf.tensor): 4D input image\n",
        "      \n",
        "    Returs:\n",
        "      detections (dict): predictions that model made\n",
        "    \"\"\"\n",
        "\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections"
      ],
      "id": "007c9c4b",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "254ccb57"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into notebooks graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    return np.array(Image.open(path))"
      ],
      "id": "254ccb57",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8flP-hjoQyU"
      },
      "source": [
        "test_dataset = fo.load_dataset(\"open-images-v6-test-1000\")"
      ],
      "id": "q8flP-hjoQyU",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0mAEmNpLbC"
      },
      "source": [
        "predictions_view = test_dataset.take(50, seed=51)"
      ],
      "id": "hH0mAEmNpLbC",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZLRLbSp-rF",
        "outputId": "b903dcd6-3252-4b5f-e0b1-bb50e1313e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import fiftyone as fo\n",
        "\n",
        "\n",
        "# Add predictions to samples\n",
        "with fo.ProgressBar() as pb:\n",
        "    for sample in pb(predictions_view):\n",
        "        # Load image\n",
        "        # image = Image.open(sample.filepath)\n",
        "        # images = tf.convert_to_tensor([image])\n",
        "        # c, h, w = images[0].shape\n",
        "\n",
        "        image_np = load_image_into_numpy_array(sample.filepath)\n",
        "        # h, w, c = image_np.shape\n",
        "        # print(image_np.shape)\n",
        "\n",
        "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "        \n",
        "\n",
        "        # Perform inference\n",
        "        preds = detect_fn(input_tensor)\n",
        "        # print(preds.keys())\n",
        "        labels = preds[\"detection_classes\"][0].numpy().astype(np.uint32)\n",
        "        scores = preds[\"detection_scores\"][0].numpy()\n",
        "        boxes = preds[\"detection_boxes\"][0].numpy()\n",
        "        # print(preds.keys())\n",
        "        # print(preds[\"num_detections\"])\n",
        "        # print(preds[\"raw_detection_scores\"])\n",
        "        # print(preds[\"detection_multiclass_scores\"])\n",
        "        # print(labels)\n",
        "        # print(scores)\n",
        "        # print(boxes)\n",
        "\n",
        "        # Convert detections to FiftyOne format\n",
        "        detections = []\n",
        "        for label, score, box in zip(labels, scores, boxes):\n",
        "            # Convert to [top-left-x, top-left-y, width, height]\n",
        "            # in relative coordinates in [0, 1] x [0, 1]\n",
        "            # if score < 0.3:\n",
        "                # print('-'*32)\n",
        "                # print(label)\n",
        "                # print(score)\n",
        "                # print(box)\n",
        "                # continue\n",
        "            # print(type(box))\n",
        "            # print(box)\n",
        "            # x1, y1, x2, y2 = box\n",
        "            # rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
        "\n",
        "            detections.append(\n",
        "                fo.Detection(\n",
        "                    label=classes[label],\n",
        "                    bounding_box=box.tolist(),\n",
        "                    confidence=score\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Save predictions to dataset\n",
        "        sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "        sample.save()"
      ],
      "id": "vgZLRLbSp-rF",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% || 50/50 [17.8s elapsed, 0s remaining, 2.9 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NyBDKC5qIZa",
        "outputId": "c688f737-3948-43ac-a058-1965948fe62b",
        "colab": {
          "resources": {
            "https://localhost:5151/polling?sessionId=b1d62294-ac33-46cb-bff1-93af57ea4638": {
              "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
              "ok": true,
              "headers": [
                [
                  "access-control-allow-headers",
                  "x-requested-with"
                ],
                [
                  "content-type",
                  "text/html; charset=UTF-8"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "session = fo.launch_app(view=predictions_view)"
      ],
      "id": "-NyBDKC5qIZa",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "#focontainer-353f822a-893c-488f-89b8-1ce45ec4124b {\n",
              "  position: relative;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-353f822a-893c-488f-89b8-1ce45ec4124b {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-353f822a-893c-488f-89b8-1ce45ec4124b:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-353f822a-893c-488f-89b8-1ce45ec4124b {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-353f822a-893c-488f-89b8-1ce45ec4124b\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-353f822a-893c-488f-89b8-1ce45ec4124b\">\n",
              "      <button id=\"foactivate-353f822a-893c-488f-89b8-1ce45ec4124b\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16efa419"
      },
      "source": [
        "**Next function is the one that you can use to run inference and plot results an an input image:**"
      ],
      "id": "16efa419"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a0e39a75"
      },
      "source": [
        "def inference_with_plot(path2images, box_th=0.25):\n",
        "    \"\"\"\n",
        "    Function that performs inference and plots resulting b-boxes\n",
        "    \n",
        "    Args:\n",
        "      path2images: an array with pathes to images\n",
        "      box_th: (float) value that defines threshold for model prediction.\n",
        "      \n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    for image_path in path2images:\n",
        "\n",
        "        print('Running inference for {}... '.format(image_path), end='')\n",
        "\n",
        "        image_np = load_image_into_numpy_array(image_path)\n",
        "        \n",
        "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "        detections = detect_fn(input_tensor)\n",
        "\n",
        "        # All outputs are batches tensors.\n",
        "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "        # We're only interested in the first num_detections.\n",
        "        num_detections = int(detections.pop('num_detections'))\n",
        "        detections = {key: value[0, :num_detections].numpy()\n",
        "                      for key, value in detections.items()}\n",
        "        \n",
        "        detections['num_detections'] = num_detections\n",
        "\n",
        "        # detection_classes should be ints.\n",
        "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "        label_id_offset = 1\n",
        "        image_np_with_detections = image_np.copy()\n",
        "\n",
        "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "                image_np_with_detections,\n",
        "                detections['detection_boxes'],\n",
        "                detections['detection_classes']+label_id_offset,\n",
        "                detections['detection_scores'],\n",
        "                category_index,\n",
        "                use_normalized_coordinates=True,\n",
        "                max_boxes_to_draw=200,\n",
        "                min_score_thresh=box_th,\n",
        "                agnostic_mode=False,\n",
        "                line_thickness=5)\n",
        "\n",
        "        plt.figure(figsize=(15,10))\n",
        "        plt.imshow(image_np_with_detections)\n",
        "        print('Done')\n",
        "    plt.show()"
      ],
      "id": "a0e39a75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07a6d10a"
      },
      "source": [
        "Next, we will define a few other supporting functions:"
      ],
      "id": "07a6d10a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff601910"
      },
      "source": [
        "def nms(rects, thd=0.5):\n",
        "    \"\"\"\n",
        "    Filter rectangles\n",
        "    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n",
        "    thd - intersection threshold (intersection divides min square of rectange)\n",
        "    \"\"\"\n",
        "    out = []\n",
        "\n",
        "    remove = [False] * len(rects)\n",
        "\n",
        "    for i in range(0, len(rects) - 1):\n",
        "        if remove[i]:\n",
        "            continue\n",
        "        inter = [0.0] * len(rects)\n",
        "        for j in range(i, len(rects)):\n",
        "            if remove[j]:\n",
        "                continue\n",
        "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
        "\n",
        "        max_prob = 0.0\n",
        "        max_idx = 0\n",
        "        for k in range(i, len(rects)):\n",
        "            if inter[k] >= thd:\n",
        "                if rects[k][1] > max_prob:\n",
        "                    max_prob = rects[k][1]\n",
        "                    max_idx = k\n",
        "\n",
        "        for k in range(i, len(rects)):\n",
        "            if (inter[k] >= thd) & (k != max_idx):\n",
        "                remove[k] = True\n",
        "\n",
        "    for k in range(0, len(rects)):\n",
        "        if not remove[k]:\n",
        "            out.append(rects[k])\n",
        "\n",
        "    boxes = [box[0] for box in out]\n",
        "    scores = [score[1] for score in out]\n",
        "    classes = [cls[2] for cls in out]\n",
        "    return boxes, scores, classes\n",
        "\n",
        "\n",
        "def intersection(rect1, rect2):\n",
        "    \"\"\"\n",
        "    Calculates square of intersection of two rectangles\n",
        "    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n",
        "    return: square of intersection\n",
        "    \"\"\"\n",
        "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
        "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
        "    overlapArea = x_overlap * y_overlap;\n",
        "    return overlapArea\n",
        "\n",
        "\n",
        "def square(rect):\n",
        "    \"\"\"\n",
        "    Calculates square of rectangle\n",
        "    \"\"\"\n",
        "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
      ],
      "id": "ff601910",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f051868c"
      },
      "source": [
        "**Next function is the one that you can use to run inference and save results into a file:**"
      ],
      "id": "f051868c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "717c2619"
      },
      "source": [
        "def inference_as_raw_output(path2images,\n",
        "                            box_th = 0.25,\n",
        "                            nms_th = 0.5,\n",
        "                            to_file = False,\n",
        "                            data = None,\n",
        "                            path2dir = False):\n",
        "    \"\"\"\n",
        "    Function that performs inference and return filtered predictions\n",
        "    \n",
        "    Args:\n",
        "      path2images: an array with pathes to images\n",
        "      box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n",
        "      nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n",
        "      to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n",
        "      path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n",
        "      data: (str) name of the dataset you passed in (e.g. test/validation)\n",
        "      path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n",
        "      \n",
        "    Returs:\n",
        "      detections (dict): filtered predictions that model made\n",
        "    \"\"\"\n",
        "\n",
        "    print(f'Current data set is {data}')\n",
        "    print(f'Ready to start inference on {len(path2images)} images!')\n",
        "    \n",
        "    for image_path in tqdm(path2images):\n",
        "        \n",
        "        if path2dir: # if a path to a directory where images are stored was passed in\n",
        "            image_path = os.path.join(path2dir, image_path.strip())\n",
        "            \n",
        "        image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "        detections = detect_fn(input_tensor)\n",
        "        \n",
        "        # checking how many detections we got\n",
        "        num_detections = int(detections.pop('num_detections'))\n",
        "        \n",
        "        # filtering out detection in order to get only the one that are indeed detections\n",
        "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
        "        \n",
        "        # detection_classes should be ints.\n",
        "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "        \n",
        "        # defining what we need from the resulting detection dict that we got from model output\n",
        "        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n",
        "        \n",
        "        # filtering out detection dict in order to get only boxes, classes and scores\n",
        "        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n",
        "        \n",
        "        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n",
        "            for key in key_of_interest:\n",
        "                scores = detections['detection_scores']\n",
        "                current_array = detections[key]\n",
        "                filtered_current_array = current_array[scores > box_th]\n",
        "                detections[key] = filtered_current_array\n",
        "        \n",
        "        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n",
        "            # creating a zip object that will contain model output info as\n",
        "            output_info = list(zip(detections['detection_boxes'],\n",
        "                                   detections['detection_scores'],\n",
        "                                   detections['detection_classes']\n",
        "                                  )\n",
        "                              )\n",
        "            boxes, scores, classes = nms(output_info)\n",
        "            \n",
        "            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n",
        "            detections['detection_scores'] = scores\n",
        "            detections['detection_classes'] = classes\n",
        "            \n",
        "        if to_file and data: # if saving to txt file was requested\n",
        "\n",
        "            image_h, image_w, _ = image_np.shape\n",
        "            file_name = f'pred_result_{data}.txt'\n",
        "            \n",
        "            line2write = list()\n",
        "            line2write.append(os.path.basename(image_path))\n",
        "            \n",
        "            with open(file_name, 'a+') as text_file:\n",
        "                # iterating over boxes\n",
        "                for b, s, c in zip(boxes, scores, classes):\n",
        "                    \n",
        "                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n",
        "                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n",
        "                    \n",
        "                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n",
        "                    line2append = ','.join([str(item) for item in list2append])\n",
        "                    \n",
        "                    line2write.append(line2append)\n",
        "                \n",
        "                line2write = ' '.join(line2write)\n",
        "                text_file.write(line2write + os.linesep)\n",
        "        \n",
        "        return detections"
      ],
      "id": "717c2619",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "384f5ac9"
      },
      "source": [
        ""
      ],
      "id": "384f5ac9",
      "execution_count": null,
      "outputs": []
    }
  ]
}